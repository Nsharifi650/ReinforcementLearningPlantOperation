{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset\n",
    "from src.data.make_dataset import generate_synthetic_data, DataRequestInput\n",
    "from datetime import date\n",
    "\n",
    "start_date = date(2015, 1, 1)  # Year, Month, Day\n",
    "end_date = date(2023, 12, 31)  # Year, Month, Day\n",
    "data_request = DataRequestInput(start_date=start_date, end_date=end_date)\n",
    "synthetic_data = generate_synthetic_data(data_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the dataset\n",
    "from src.data.preprocess import preprocess_data, PreprocessConfig\n",
    "\n",
    "preprocess_config = PreprocessConfig()  # using the default set values!\n",
    "train_data, val_data, test_data = preprocess_data(config=preprocess_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the environment And training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.Model_TrainTest import Model_train, ModelEvaluationConfig\n",
    "\n",
    "actions_list = {\n",
    "    \"main_turbine_output\": [0, 500],\n",
    "    \"secondary_turbine_output\": [0, 500],\n",
    "    \"current_fuel_mixture_ratio\": [0, 1],\n",
    "    \"generator_excitation\": [0, 1],\n",
    "    \"emissions_control_intensity\": [0, 1],\n",
    "}\n",
    "\n",
    "internal_states_initialValue = {\n",
    "    \"current_power_output\": 0,\n",
    "    \"fuel_consumption_rate\": 0,\n",
    "    \"emissions_levels\": 0,\n",
    "    \"current_operating_costs\": 0,\n",
    "    \"emissions_quota\": 40000,\n",
    "    \"hours_main_turbine_since_maintenance\": 0,\n",
    "    \"hours_secondary_turbine_since_maintenance\": 0,\n",
    "    \"current_fuel_mixture_ratio\": 0.5,\n",
    "    \"Initial_storage\": 0,\n",
    "}\n",
    "\n",
    "environment_variables = {\n",
    "    \"Energy_storage_capacity\": 15000,\n",
    "    \"Turbine_maintenance_time\": 10,\n",
    "    \"Turbine_use_b4_maintenance\": 100,\n",
    "}\n",
    "\n",
    "Model_config = ModelEvaluationConfig(\n",
    "    agent_possible_actions=actions_list,\n",
    "    train_total_timesteps=350000,\n",
    "    internalStates_InitialVal=internal_states_initialValue,\n",
    "    environment_variables=environment_variables,\n",
    "    test_timesteps=1000,\n",
    ")\n",
    "\n",
    "TheMatrix = Model_train(Model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the agent\n",
    "train_environment, val_environment, agent, Reward, accumulative_reward = (\n",
    "    TheMatrix.train_agent()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_results(rewards):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards)\n",
    "    plt.title(\"Cumulative Reward over Time\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_results(accumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.training.Model_TrainTest import Model_train, ModelEvaluationConfig\n",
    "\n",
    "\n",
    "# Model_config =ModelEvaluationConfig(\n",
    "#     agent_possible_actions=actions_list,\n",
    "#     train_total_timesteps=500000,\n",
    "#     internalStates_InitialVal = internal_states_initialValue,\n",
    "#     environment_variables=environment_variables,\n",
    "#     test_timesteps=2000,\n",
    "# )\n",
    "\n",
    "# TheMatrix = Model_train(Model_config)\n",
    "\n",
    "test_accum_rewards, test_rewards, test_observation_space = TheMatrix.test_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(rewards):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards)\n",
    "    plt.title(\"Cumulative Reward over Time\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_results(test_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "internal_state_variable = []\n",
    "variable_index = 18\n",
    "\n",
    "for jj in range(len(test_observation_space)):\n",
    "    step_observation = test_observation_space[jj]\n",
    "    # print(step_observation.shape)\n",
    "    internal_state_variable.append(step_observation[0, variable_index])\n",
    "internal_state_variable = np.array(internal_state_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(rewards):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards)\n",
    "    plt.title(\"Cumulative Reward over Time\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_results(internal_state_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING RESULTS\n",
    "# INTERNAL STATES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "internal_states = []\n",
    "for jj in range(len(test_observation_space)):\n",
    "    observation_step = test_observation_space[jj]\n",
    "    internal_states.append(observation_step[0, :])\n",
    "\n",
    "variable_id_list = [\n",
    "    f\"Internal_variable_{ii}\" for ii in range(len(observation_step[0, :]))\n",
    "]\n",
    "internal_states_pd = pd.DataFrame(internal_states, columns=variable_id_list)\n",
    "\n",
    "# REWARDS AND ACCUMMULATIVE REWARDS\n",
    "combined_rewards = np.transpose(np.array([test_rewards, test_accum_rewards]))\n",
    "rewards_pd = pd.DataFrame(combined_rewards, columns=[\"rewards\", \"accum_rewards\"])\n",
    "\n",
    "complete_data = pd.concat([internal_states_pd, rewards_pd], axis=1)\n",
    "\n",
    "file_name = \"PPOAgent_350000Steps_results.csv\"\n",
    "complete_data.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "file_path = \"data/processed/train.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "columns_to_normalize = df.columns[df.columns.get_loc(\"index\") + 1 :]\n",
    "scaler = MinMaxScaler()\n",
    "df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
    "\n",
    "df = df.iloc[:50, :]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for col in columns_to_normalize:\n",
    "    fig.add_trace(go.Scatter(x=df[\"index\"], y=df[col], mode=\"lines\", name=col))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Normalised Data  as a function of time\",\n",
    "    xaxis_title=\"time (hr)\",\n",
    "    yaxis_title=\"Normalized Value\",\n",
    "    width=1000,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCUMMULATIVE REWARD\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "trained_Agent = pd.read_csv(\"PPOAgent_350000Steps_results.csv\")\n",
    "initial_Agent = pd.read_csv(\"PPOAgent_10000Steps_results.csv\")\n",
    "\n",
    "# parameter to plot\n",
    "parameter = \"accum_rewards\"\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=trained_Agent.index,\n",
    "        y=trained_Agent[parameter],\n",
    "        mode=\"lines\",\n",
    "        name=\"500k steps trained\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=trained_Agent.index,\n",
    "        y=initial_Agent[parameter],\n",
    "        mode=\"lines\",\n",
    "        name=\"10k steps trained\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"Accummulated reward\",\n",
    "    xaxis_title=\"time (hrs)\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    # yaxis=dict(range=[-1e8, 1e7]),\n",
    "    # xaxis=dict(range=[0, 150])\n",
    "    # yaxis_type = \"log\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPERATIONAL PARAMETERS\n",
    "\n",
    "# parameter to plot\n",
    "parameter = \"Internal_variable_18\"\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=trained_Agent.index,\n",
    "        y=trained_Agent[parameter],\n",
    "        mode=\"lines\",\n",
    "        name=\"500k steps trained\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=trained_Agent.index,\n",
    "        y=initial_Agent[parameter],\n",
    "        mode=\"lines\",\n",
    "        name=\"10k steps trained\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"Accummulated reward\",\n",
    "    xaxis_title=\"time (hrs)\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    # yaxis_type = \"log\"\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
